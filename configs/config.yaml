# pytorch_lightning==1.7.6

# Set to an int to run seed_everything with this value before classes instantiation (type: Union[int, null], default: 42)
seed_everything: 42

# optimizer:
#   class_path: torch.optim.Adam
#   init_args:
#     lr: 0.01
# lr_scheduler:
#   class_path: torch.optim.lr_scheduler.ExponentialLR
#   init_args:
#     gamma: 0.1

# <class 'training.sdf_experiment.SdfExperiment'>
model:

  #   (required, type: <class 'Siren'>, known subclasses: models.siren.Siren)
  sdf_model:
    class_path: models.siren.Siren
    init_args: 
      input_dim: 3
      hidden_dim: 256
      hidden_layers: 4
      output_dim: 1
      outermost_linear: True
      first_omega_0: 50.
      hidden_omega_0: 60.
      use_geometric_initialization: False
      modulation_type: FILM
      bias_init_scheme: HE_UNIFORM
      self_modulate: []

  #   (required, type: str)
  mesh_path: data/frazer-nash-super-sport-1929.obj

  #   (type: int, default: 1024)
  batch_size: 10000

  #   (type: float, default: 10)
  level_set_loss_weight: 100.0

  #   (type: float, default: 1)
  eikonal_loss_weight: 2.

  #   (type: float, default: 1)
  grad_direction_loss_weight: 3.0

  offsurface_loss_weight: 30.0

  offsurface_loss_margin: 0.0025

  high_order_loss_weight: 0.0

  high_order_loss_type: Div

  apply_high_order_loss_to: OffSurface

  learning_rate: 0.00025

  learning_rate_decay: 0.97

  lr_warmup_steps: 300


# Path/URL of the checkpoint from which training is resumed. If there is
# no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint,
# training will start from the beginning of the next epoch. (type: Optional[str], default: null)
ckpt_path:


# Customize every aspect of training via flags
trainer:

  logger: true

  enable_checkpointing: true

  # Add a callback or list of callbacks.
  # Default: ``None``. (type: Union[List[Callback], Callback, null], default: null, known subclasses: pytorch_lightning.Callback, pytorch_lightning.callbacks.Checkpoint, pytorch_lightning.callbacks.ModelCheckpoint, pytorch_lightning.callbacks.DeviceStatsMonitor, pytorch_lightning.callbacks.EarlyStopping, pytorch_lightning.callbacks.BaseFinetuning, pytorch_lightning.callbacks.BackboneFinetuning, pytorch_lightning.callbacks.GradientAccumulationScheduler, pytorch_lightning.callbacks.LambdaCallback, pytorch_lightning.callbacks.LearningRateMonitor, pytorch_lightning.callbacks.ModelSummary, pytorch_lightning.callbacks.RichModelSummary, pytorch_lightning.callbacks.BasePredictionWriter, pytorch_lightning.callbacks.ProgressBarBase, pytorch_lightning.callbacks.RichProgressBar, pytorch_lightning.callbacks.TQDMProgressBar, pytorch_lightning.callbacks.Timer, pytorch_lightning.callbacks.ModelPruning, pytorch_lightning.callbacks.QuantizationAwareTraining, pytorch_lightning.callbacks.StochasticWeightAveraging, pytorch_lightning.cli.SaveConfigCallback, pytorch_lightning.utilities.cli.SaveConfigCallback, __main__.VisualizationCalback)
  callbacks: 
    - train_sdf.VisualizationCalback
    - train_sdf.ActivationDistributionCalback
    # - pytorch_lightning.callbacks.StochasticWeightAveraging
    - pytorch_lightning.callbacks.LearningRateMonitor
    # - train_sdf.SirenFrequencyCalback

  # Default path for logs and weights when no logger/ckpt_callback passed.
  # Default: ``os.getcwd()``.
  # Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/' (type: Optional[str], default: null)
  default_root_dir:

  # The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables
  # gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.
  # Default: ``None``. (type: Union[int, float, null], default: null)
  gradient_clip_val:

  # The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm="value"``
  # to clip by value, and ``gradient_clip_algorithm="norm"`` to clip by norm. By default it will
  # be set to ``"norm"``. (type: Optional[str], default: null)
  gradient_clip_algorithm:

  # Number of GPU nodes for distributed training.
  # Default: ``1``. (type: int, default: 1)
  num_nodes: 1

  # Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`,
  # based on the accelerator type. (type: Union[List[int], str, int, null], default: null)
  devices: 1


  # If enabled and ``gpus`` or ``devices`` is an integer, pick available
  # gpus automatically. This is especially useful when
  # GPUs are configured to be in "exclusive mode", such
  # that only one process at a time can access them.
  # Default: ``False``. (type: bool, default: False)
  auto_select_gpus: true


  # Whether to enable to progress bar by default.
  # Default: ``True``. (type: bool, default: True)
  enable_progress_bar: true

  # Overfit a fraction of training/validation data (float) or a set number of batches (int).
  # Default: ``0.0``. (type: Union[int, float], default: 0.0)
  overfit_batches: 0.0

  # -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. If using
  # Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.
  # Default: ``-1``. (type: Union[int, float, str], default: -1)
  track_grad_norm: 2

  # Perform a validation loop every after every `N` training epochs. If ``None``,
  # validation will be done solely based on the number of training batches, requiring ``val_check_interval``
  # to be an integer value.
  # Default: ``1``. (type: Optional[int], default: 1)
  check_val_every_n_epoch: 1

  # Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)
  # of train, val and test to find any bugs (ie: a sort of unit test).
  # Default: ``False``. (type: Union[int, bool], default: False)
  fast_dev_run: false

  # Accumulates grads every k batches or as set up in the dict.
  # Default: ``None``. (type: Union[int, Dict[int, int], null], default: null)
  accumulate_grad_batches:

  # Stop training once this number of epochs is reached. Disabled by default (None).
  # If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.
  # To enable infinite training, set ``max_epochs = -1``. (type: Optional[int], default: null)
  max_epochs: 128

  # Force training for at least these many epochs. Disabled by default (None). (type: Optional[int], default: null)
  min_epochs:

  # Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``
  # and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set
  # ``max_epochs`` to ``-1``. (type: int, default: -1)
  max_steps: -1

  # Force training for at least these number of steps. Disabled by default (``None``). (type: Optional[int], default: null)
  min_steps:

  # Stop training after this amount of time has passed. Disabled by default (``None``).
  # The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a
  # :class:`datetime.timedelta`, or a dictionary with keys that will be passed to
  # :class:`datetime.timedelta`. (type: Union[str, timedelta, Dict[str, int], null], default: null)
  max_time:

  # How much of training dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_train_batches:

  # How much of validation dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_val_batches:

  # How much of test dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_test_batches:

  # How much of prediction dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_predict_batches:

  # How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check
  # after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training
  # batches. An ``int`` value can only be higher than the number of training batches when
  # ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches
  # across epochs or during iteration-based training.
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  val_check_interval:

  # How often to log within steps.
  # Default: ``50``. (type: int, default: 50)
  log_every_n_steps: 50

  # Supports passing different accelerator types ("cpu", "gpu", "tpu", "ipu", "hpu", "mps, "auto")
  # as well as custom accelerator instances.

  # .. deprecated:: v1.5
  #     Passing training strategies (e.g., 'ddp') to ``accelerator`` has been deprecated in v1.5.0
  #     and will be removed in v1.7.0. Please use the ``strategy`` argument instead. (type: Union[str, Accelerator, null], default: null, known subclasses: pytorch_lightning.accelerators.CUDAAccelerator, pytorch_lightning.accelerators.GPUAccelerator, pytorch_lightning.accelerators.IPUAccelerator, pytorch_lightning.accelerators.CPUAccelerator, pytorch_lightning.accelerators.HPUAccelerator, pytorch_lightning.accelerators.MPSAccelerator, pytorch_lightning.accelerators.TPUAccelerator)
  accelerator: gpu

  # Supports different training strategies with aliases
  # as well custom strategies.
  # Default: ``None``. (type: Union[str, Strategy, null], default: null, known subclasses: pytorch_lightning.strategies.DDPStrategy, pytorch_lightning.strategies.BaguaStrategy, pytorch_lightning.strategies.DeepSpeedStrategy, pytorch_lightning.plugins.DeepSpeedPlugin, pytorch_lightning.strategies.DDPFullyShardedStrategy, pytorch_lightning.plugins.DDPFullyShardedPlugin, pytorch_lightning.strategies.HPUParallelStrategy, pytorch_lightning.strategies.DDPShardedStrategy, pytorch_lightning.plugins.DDPShardedPlugin, pytorch_lightning.plugins.DDPPlugin, pytorch_lightning.strategies.DDPSpawnStrategy, pytorch_lightning.strategies.DDPSpawnShardedStrategy, pytorch_lightning.plugins.DDPSpawnShardedPlugin, pytorch_lightning.strategies.TPUSpawnStrategy, pytorch_lightning.plugins.TPUSpawnPlugin, pytorch_lightning.plugins.DDPSpawnPlugin, pytorch_lightning.strategies.DataParallelStrategy, pytorch_lightning.plugins.DataParallelPlugin, pytorch_lightning.strategies.DDPFullyShardedNativeStrategy, pytorch_lightning.strategies.HorovodStrategy, pytorch_lightning.plugins.HorovodPlugin, pytorch_lightning.strategies.IPUStrategy, pytorch_lightning.plugins.IPUPlugin, pytorch_lightning.strategies.HivemindStrategy, pytorch_lightning.strategies.SingleDeviceStrategy, pytorch_lightning.strategies.SingleHPUStrategy, pytorch_lightning.strategies.SingleTPUStrategy, pytorch_lightning.plugins.SingleTPUPlugin, pytorch_lightning.plugins.SingleDevicePlugin)
  strategy:

  # Synchronize batch norm layers between process groups/whole world.
  # Default: ``False``. (type: bool, default: False)
  sync_batchnorm: false

  # Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16).
  # Can be used on CPU, GPU, TPUs, HPUs or IPUs.
  # Default: ``32``. (type: Union[int, str], default: 32)
  precision: 32

  # Whether to enable model summarization by default.
  # Default: ``True``. (type: bool, default: True)
  enable_model_summary: true

  # Sanity check runs n validation batches before starting the training routine.
  # Set it to `-1` to run all batches in all validation dataloaders.
  # Default: ``2``. (type: int, default: 2)
  num_sanity_val_steps: 2


  # To profile individual steps during training and assist in identifying bottlenecks.
  # Default: ``None``. (type: Union[Profiler, str, null], default: null, known subclasses: pytorch_lightning.profilers.AdvancedProfiler, pytorch_lightning.profilers.PassThroughProfiler, pytorch_lightning.profilers.PyTorchProfiler, pytorch_lightning.profilers.SimpleProfiler, pytorch_lightning.profilers.XLAProfiler)
  profiler:

  # The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.
  # The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used
  # (``False`` if not manually set). If :paramref:`~pytorch_lightning.trainer.Trainer.deterministic` is set
  # to ``True``, this will default to ``False``. Override to manually set a different value.
  # Default: ``None``. (type: Optional[bool], default: null)
  benchmark:

  # If ``True``, sets whether PyTorch operations must use deterministic algorithms.
  # Set to ``"warn"`` to use deterministic algorithms whenever possible, throwing warnings on operations
  # that don't support deterministic mode (requires PyTorch 1.11+). If not set, defaults to ``False``.
  # Default: ``None``. (type: Union[bool, Literal['warn'], null], default: null)
  deterministic:

  # Set to a non-negative integer to reload dataloaders every n epochs.
  # Default: ``0``. (type: int, default: 0)
  reload_dataloaders_every_n_epochs: 0

  # If set to True, will make trainer.tune() run a learning rate finder,
  # trying to optimize initial learning for faster convergence. trainer.tune() method will
  # set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.
  # To use a different key set a string instead of True with the key name.
  # Default: ``False``. (type: Union[bool, str], default: False)
  auto_lr_find: false

  # Explicitly enables or disables sampler replacement. If not specified this
  # will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for
  # train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,
  # you can set ``replace_sampler_ddp=False`` and add your own distributed sampler. (type: bool, default: True)
  replace_sampler_ddp: true

  # Enable anomaly detection for the autograd engine.
  # Default: ``False``. (type: bool, default: False)
  detect_anomaly: false

  # If set to True, will `initially` run a batch size
  # finder trying to find the largest batch size that fits into memory.
  # The result will be stored in self.batch_size in the LightningModule.
  # Additionally, can be set to either `power` that estimates the batch size through
  # a power search or `binsearch` that estimates the batch size through a binary search.
  # Default: ``False``. (type: Union[str, bool], default: False)
  auto_scale_batch_size: false

  # Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.
  # Default: ``None``. (type: Union[Strategy, PrecisionPlugin, ClusterEnvironment, CheckpointIO, LayerSync, str, List[Union[Strategy, PrecisionPlugin, ClusterEnvironment, CheckpointIO, LayerSync, str]], null], default: null, known subclasses: pytorch_lightning.strategies.DDPStrategy, pytorch_lightning.strategies.BaguaStrategy, pytorch_lightning.strategies.DeepSpeedStrategy, pytorch_lightning.plugins.DeepSpeedPlugin, pytorch_lightning.strategies.DDPFullyShardedStrategy, pytorch_lightning.plugins.DDPFullyShardedPlugin, pytorch_lightning.strategies.HPUParallelStrategy, pytorch_lightning.strategies.DDPShardedStrategy, pytorch_lightning.plugins.DDPShardedPlugin, pytorch_lightning.plugins.DDPPlugin, pytorch_lightning.strategies.DDPSpawnStrategy, pytorch_lightning.strategies.DDPSpawnShardedStrategy, pytorch_lightning.plugins.DDPSpawnShardedPlugin, pytorch_lightning.strategies.TPUSpawnStrategy, pytorch_lightning.plugins.TPUSpawnPlugin, pytorch_lightning.plugins.DDPSpawnPlugin, pytorch_lightning.strategies.DataParallelStrategy, pytorch_lightning.plugins.DataParallelPlugin, pytorch_lightning.strategies.DDPFullyShardedNativeStrategy, pytorch_lightning.strategies.HorovodStrategy, pytorch_lightning.plugins.HorovodPlugin, pytorch_lightning.strategies.IPUStrategy, pytorch_lightning.plugins.IPUPlugin, pytorch_lightning.strategies.HivemindStrategy, pytorch_lightning.strategies.SingleDeviceStrategy, pytorch_lightning.strategies.SingleHPUStrategy, pytorch_lightning.strategies.SingleTPUStrategy, pytorch_lightning.plugins.SingleTPUPlugin, pytorch_lightning.plugins.SingleDevicePlugin, pytorch_lightning.plugins.PrecisionPlugin, pytorch_lightning.plugins.precision.MixedPrecisionPlugin, pytorch_lightning.plugins.ApexMixedPrecisionPlugin, pytorch_lightning.plugins.NativeMixedPrecisionPlugin, pytorch_lightning.plugins.FullyShardedNativeNativeMixedPrecisionPlugin, pytorch_lightning.plugins.ShardedNativeMixedPrecisionPlugin, pytorch_lightning.plugins.FullyShardedNativeMixedPrecisionPlugin, pytorch_lightning.plugins.DeepSpeedPrecisionPlugin, pytorch_lightning.plugins.DoublePrecisionPlugin, pytorch_lightning.plugins.HPUPrecisionPlugin, pytorch_lightning.plugins.IPUPrecisionPlugin, pytorch_lightning.plugins.TPUPrecisionPlugin, pytorch_lightning.plugins.TPUBf16PrecisionPlugin, pytorch_lightning.plugins.environments.BaguaEnvironment, pytorch_lightning.plugins.environments.KubeflowEnvironment, pytorch_lightning.plugins.environments.LightningEnvironment, pytorch_lightning.plugins.environments.LSFEnvironment, pytorch_lightning.plugins.environments.SLURMEnvironment, pytorch_lightning.plugins.environments.TorchElasticEnvironment, pytorch_lightning.plugins.environments.XLAEnvironment, pytorch_lightning.plugins.AsyncCheckpointIO, pytorch_lightning.plugins.TorchCheckpointIO, pytorch_lightning.plugins.HPUCheckpointIO, pytorch_lightning.plugins.XLACheckpointIO, pytorch_lightning.plugins.NativeSyncBatchNorm)
  plugins:

  # The mixed precision backend to use ("native" or "apex").
  # Default: ``'native''``. (type: str, default: native)
  amp_backend: native

  # The optimization level to use (O1, O2, etc...). By default it will be set to "O2"
  # if ``amp_backend`` is set to "apex". (type: Optional[str], default: null)
  amp_level:

  # Whether to force internal logged metrics to be moved to cpu.
  # This can save some gpu memory, but can make training slower. Use with attention.
  # Default: ``False``. (type: bool, default: False)
  move_metrics_to_cpu: false

  # How to loop over the datasets when there are multiple train loaders.
  # In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,
  # and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets
  # reload when reaching the minimum length of datasets.
  # Default: ``"max_size_cycle"``. (type: str, default: max_size_cycle)
  multiple_trainloader_mode: max_size_cycle

