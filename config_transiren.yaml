
# Set to an int to run seed_everything with this value before classes instantiation (type: Union[int, null], default: 42)
seed_everything: 42


# <class 'training.sdf_experiment.SdfExperiment'>
model:

  #   (required, type: <class 'Siren'>, known subclasses: models.siren.Siren)
  sdf_model:
    class_path: models.siren.TransSiren
    init_args: 
      in_features: 3
      hidden_features: 256
      hidden_layers: 4
      out_features: 1
      outermost_linear: True
      first_omega_0: 50.
      hidden_omega_0: 60.
      linear_map_output: False

  #   (required, type: str)
  mesh_path: /home/duke/Documents/artefacts/3d_models/frazer-nash-super-sport-1929.obj

  #   (type: int, default: 1024)
  batch_size: 6000

  #   (type: float, default: 10)
  level_set_loss_weight: 100.0

  #   (type: float, default: 1)
  eikonal_loss_weight: 2.

  #   (type: float, default: 1)
  grad_direction_loss_weight: 3.0

  #   (type: bool, default: True)
  enforce_eikonality: true

  offsurface_loss_weight: 30.0

  divergence_loss_weitht: 0.005

  learning_rate: 0.00003

  kernel_coefficient: 100

# Path/URL of the checkpoint from which training is resumed. If there is
# no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint,
# training will start from the beginning of the next epoch. (type: Union[str, null], default: null)
ckpt_path: 

# Customize every aspect of training via flags
trainer:

  # Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses
  # the default ``TensorBoardLogger``. ``False`` will disable logging. If multiple loggers are
  # provided and the `save_dir` property of that logger is not set, local files (checkpoints,
  # profiler traces, etc.) are saved in ``default_root_dir`` rather than in the ``log_dir`` of any
  # of the individual loggers.
  # Default: ``True``. (type: Union[LightningLoggerBase, Iterable[LightningLoggerBase], bool], default: True, known subclasses: pytorch_lightning.loggers.LoggerCollection, pytorch_lightning.loggers.base.DummyLogger, pytorch_lightning.loggers.CSVLogger, pytorch_lightning.loggers.TensorBoardLogger, pytorch_lightning.loggers.CometLogger, pytorch_lightning.loggers.MLFlowLogger, pytorch_lightning.loggers.NeptuneLogger, pytorch_lightning.loggers.TestTubeLogger, pytorch_lightning.loggers.WandbLogger)
  logger: true


  # If ``True``, enable checkpointing.
  # It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in
  # :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.
  # Default: ``True``. (type: bool, default: True)
  enable_checkpointing: true

  # Add a callback or list of callbacks.
  # Default: ``None``. (type: Union[List[Callback], Callback, null], default: null, known subclasses: pytorch_lightning.Callback, pytorch_lightning.callbacks.DeviceStatsMonitor, pytorch_lightning.callbacks.EarlyStopping, pytorch_lightning.callbacks.BaseFinetuning, pytorch_lightning.callbacks.BackboneFinetuning, pytorch_lightning.callbacks.GPUStatsMonitor, pytorch_lightning.callbacks.GradientAccumulationScheduler, pytorch_lightning.callbacks.LambdaCallback, pytorch_lightning.callbacks.LearningRateMonitor, pytorch_lightning.callbacks.ModelCheckpoint, pytorch_lightning.callbacks.ModelSummary, pytorch_lightning.callbacks.RichModelSummary, pytorch_lightning.callbacks.BasePredictionWriter, pytorch_lightning.callbacks.ProgressBarBase, pytorch_lightning.callbacks.TQDMProgressBar, pytorch_lightning.callbacks.ProgressBar, pytorch_lightning.callbacks.RichProgressBar, pytorch_lightning.callbacks.Timer, pytorch_lightning.callbacks.ModelPruning, pytorch_lightning.callbacks.QuantizationAwareTraining, pytorch_lightning.callbacks.StochasticWeightAveraging, pytorch_lightning.callbacks.XLAStatsMonitor, pytorch_lightning.utilities.cli.SaveConfigCallback)
  callbacks: 
    - train_sdf.VisualizationCalback
    # - pytorch_lightning.callbacks.StochasticWeightAveraging
    - pytorch_lightning.callbacks.LearningRateMonitor


  # Default path for logs and weights when no logger/ckpt_callback passed.
  # Default: ``os.getcwd()``.
  # Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/' (type: Union[str, null], default: null)
  default_root_dir:

  # The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables
  # gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.
  # Default: ``None``. (type: Union[int, float, null], default: null)
  gradient_clip_val:

  # The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm="value"``
  # to clip by value, and ``gradient_clip_algorithm="norm"`` to clip by norm. By default it will
  # be set to ``"norm"``. (type: Union[str, null], default: null)
  gradient_clip_algorithm:

  # Orders the progress bar when running multiple models on same machine.
  # .. deprecated:: v1.5
  #     ``process_position`` has been deprecated in v1.5 and will be removed in v1.7.
  #     Please pass :class:`~pytorch_lightning.callbacks.progress.TQDMProgressBar` with ``process_position``
  #     directly to the Trainer's ``callbacks`` argument instead. (type: int, default: 0)
  process_position: 0

  # Number of GPU nodes for distributed training.
  # Default: ``1``. (type: int, default: 1)
  num_nodes: 1

  # Number of processes for distributed training with ``accelerator="cpu"``.
  # Default: ``1``. (type: Union[int, null], default: null)
  num_processes:

  # Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`,
  # based on the accelerator type. (type: Union[List[int], str, int, null], default: null)
  devices:

  # Number of GPUs to train on (int) or which GPUs to train on (list or str) applied per node
  # Default: ``None``. (type: Union[List[int], str, int, null], default: null)
  gpus: 1

  # If enabled and ``gpus`` or ``devices`` is an integer, pick available
  # gpus automatically. This is especially useful when
  # GPUs are configured to be in "exclusive mode", such
  # that only one process at a time can access them.
  # Default: ``False``. (type: bool, default: False)
  auto_select_gpus: true

  # How many TPU cores to train on (1 or 8) / Single TPU to train on (1)
  # Default: ``None``. (type: Union[List[int], str, int, null], default: null)
  tpu_cores:

  # How many IPUs to train on.
  # Default: ``None``. (type: Union[int, null], default: null)
  ipus:


  # Whether to enable to progress bar by default.
  # Default: ``False``. (type: bool, default: True)
  enable_progress_bar: true

  # Overfit a fraction of training data (float) or a set number of batches (int).
  # Default: ``0.0``. (type: Union[int, float], default: 0.0)
  overfit_batches: 0.0

  # -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. If using
  # Automatic Mixed Precision (AMP), the gradients will be unscaled before logging them.
  # Default: ``-1``. (type: Union[int, float, str], default: -1)
  track_grad_norm: -1

  # Check val every n train epochs.
  # Default: ``1``. (type: int, default: 1)
  check_val_every_n_epoch: 1

  # Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)
  # of train, val and test to find any bugs (ie: a sort of unit test).
  # Default: ``False``. (type: Union[int, bool], default: False)
  fast_dev_run: false

  # Accumulates grads every k batches or as set up in the dict.
  # Default: ``None``. (type: Union[int, Dict[int, int], null], default: null)
  accumulate_grad_batches:

  # Stop training once this number of epochs is reached. Disabled by default (None).
  # If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.
  # To enable infinite training, set ``max_epochs = -1``. (type: Union[int, null], default: null)
  max_epochs: 100

  # Force training for at least these many epochs. Disabled by default (None). (type: Union[int, null], default: null)
  min_epochs:

  # Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``
  # and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set
  # ``max_epochs`` to ``-1``. (type: int, default: -1)
  max_steps: -1

  # Force training for at least these number of steps. Disabled by default (``None``). (type: Union[int, null], default: null)
  min_steps:

  # Stop training after this amount of time has passed. Disabled by default (``None``).
  # The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a
  # :class:`datetime.timedelta`, or a dictionary with keys that will be passed to
  # :class:`datetime.timedelta`. (type: Union[str, timedelta, Dict[str, int], null], default: null)
  max_time:

  # How much of training dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_train_batches:

  # How much of validation dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_val_batches:

  # How much of test dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_test_batches:

  # How much of prediction dataset to check (float = fraction, int = num_batches).
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  limit_predict_batches:

  # How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check
  # after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training
  # batches.
  # Default: ``1.0``. (type: Union[int, float, null], default: null)
  val_check_interval:

  # How often to flush logs to disk (defaults to every 100 steps).
  # .. deprecated:: v1.5
  #     ``flush_logs_every_n_steps`` has been deprecated in v1.5 and will be removed in v1.7.
  #     Please configure flushing directly in the logger instead. (type: Union[int, null], default: null)
  flush_logs_every_n_steps:

  # How often to log within steps.
  # Default: ``50``. (type: int, default: 50)
  log_every_n_steps: 50

  # Supports passing different accelerator types ("cpu", "gpu", "tpu", "ipu", "hpu", "auto")
  # as well as custom accelerator instances.

  # .. deprecated:: v1.5
  #     Passing training strategies (e.g., 'ddp') to ``accelerator`` has been deprecated in v1.5.0
  #     and will be removed in v1.7.0. Please use the ``strategy`` argument instead. (type: Union[str, Accelerator, null], default: null, known subclasses: pytorch_lightning.accelerators.IPUAccelerator, pytorch_lightning.accelerators.CPUAccelerator, pytorch_lightning.accelerators.GPUAccelerator, pytorch_lightning.accelerators.HPUAccelerator, pytorch_lightning.accelerators.TPUAccelerator)
  accelerator:

  # Supports different training strategies with aliases
  # as well custom strategies.
  # Default: ``None``. (type: Union[str, Strategy, null], default: null, known subclasses: pytorch_lightning.strategies.DDPStrategy, pytorch_lightning.strategies.BaguaStrategy, pytorch_lightning.strategies.DDP2Strategy, pytorch_lightning.plugins.DDP2Plugin, pytorch_lightning.strategies.DeepSpeedStrategy, pytorch_lightning.plugins.DeepSpeedPlugin, pytorch_lightning.strategies.DDPFullyShardedStrategy, pytorch_lightning.plugins.DDPFullyShardedPlugin, pytorch_lightning.strategies.HPUParallelStrategy, pytorch_lightning.strategies.DDPShardedStrategy, pytorch_lightning.plugins.DDPShardedPlugin, pytorch_lightning.plugins.DDPPlugin, pytorch_lightning.strategies.DDPSpawnStrategy, pytorch_lightning.strategies.DDPSpawnShardedStrategy, pytorch_lightning.plugins.DDPSpawnShardedPlugin, pytorch_lightning.strategies.TPUSpawnStrategy, pytorch_lightning.plugins.TPUSpawnPlugin, pytorch_lightning.plugins.DDPSpawnPlugin, pytorch_lightning.strategies.DataParallelStrategy, pytorch_lightning.plugins.DataParallelPlugin, pytorch_lightning.strategies.HorovodStrategy, pytorch_lightning.plugins.HorovodPlugin, pytorch_lightning.strategies.IPUStrategy, pytorch_lightning.plugins.IPUPlugin, pytorch_lightning.strategies.SingleDeviceStrategy, pytorch_lightning.strategies.SingleHPUStrategy, pytorch_lightning.strategies.SingleTPUStrategy, pytorch_lightning.plugins.SingleTPUPlugin, pytorch_lightning.plugins.SingleDevicePlugin)
  strategy:

  # Synchronize batch norm layers between process groups/whole world.
  # Default: ``False``. (type: bool, default: False)
  sync_batchnorm: false

  # Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16).
  # Can be used on CPU, GPU, TPUs, HPUs or IPUs.
  # Default: ``32``. (type: Union[int, str], default: 32)
  precision: 32

  # Whether to enable model summarization by default.
  # Default: ``True``. (type: bool, default: True)
  enable_model_summary: true

  # Prints a summary of the weights when training begins.
  # .. deprecated:: v1.5
  #     ``weights_summary`` has been deprecated in v1.5 and will be removed in v1.7.
  #     To disable the summary, pass ``enable_model_summary = False`` to the Trainer.
  #     To customize the summary, pass :class:`~pytorch_lightning.callbacks.model_summary.ModelSummary`
  #     directly to the Trainer's ``callbacks`` argument. (type: Union[str, null], default: top)
  weights_summary: top

  # Where to save weights if specified. Will override default_root_dir
  # for checkpoints only. Use this if for whatever reason you need the checkpoints
  # stored in a different place than the logs written in `default_root_dir`.
  # Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'
  # Defaults to `default_root_dir`.

  # .. deprecated:: v1.6
  #     ``weights_save_path`` has been deprecated in v1.6 and will be removed in v1.8. Please pass
  #     ``dirpath`` directly to the :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`
  #     callback. (type: Union[str, null], default: null)
  weights_save_path:

  # Sanity check runs n validation batches before starting the training routine.
  # Set it to `-1` to run all batches in all validation dataloaders.
  # Default: ``2``. (type: int, default: 2)
  num_sanity_val_steps: 2

  # Path/URL of the checkpoint from which training is resumed. If there is
  # no checkpoint file at the path, an exception is raised. If resuming from mid-epoch checkpoint,
  # training will start from the beginning of the next epoch.

  # .. deprecated:: v1.5
  #     ``resume_from_checkpoint`` is deprecated in v1.5 and will be removed in v2.0.
  #     Please pass the path to ``Trainer.fit(..., ckpt_path=...)`` instead. (type: Union[str, Path, null], default: null, known subclasses: pathlib.Path, pathlib.PosixPath, pathlib.WindowsPath)
  resume_from_checkpoint:

  # To profile individual steps during training and assist in identifying bottlenecks.
  # Default: ``None``. (type: Union[Profiler, str, null], default: null, known subclasses: pytorch_lightning.profiler.AdvancedProfiler, pytorch_lightning.profiler.PassThroughProfiler, pytorch_lightning.profiler.PyTorchProfiler, pytorch_lightning.profiler.SimpleProfiler, pytorch_lightning.profiler.XLAProfiler)
  profiler:

  # The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.
  # The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used
  # (``False`` if not manually set). If :paramref:`~pytorch_lightning.trainer.Trainer.deterministic` is set
  # to ``True``, this will default to ``False``. Override to manually set a different value.
  # Default: ``None``. (type: Union[bool, null], default: null)
  benchmark:

  # If ``True``, sets whether PyTorch operations must use deterministic algorithms.
  # If not set, defaults to ``False``.
  # Default: ``None``. (type: Union[bool, null], default: null)
  deterministic:

  # Set to a non-negative integer to reload dataloaders every n epochs.
  # Default: ``0``. (type: int, default: 0)
  reload_dataloaders_every_n_epochs: 0

  # If set to True, will make trainer.tune() run a learning rate finder,
  # trying to optimize initial learning for faster convergence. trainer.tune() method will
  # set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.
  # To use a different key set a string instead of True with the key name.
  # Default: ``False``. (type: Union[bool, str], default: False)
  auto_lr_find: false

  # Explicitly enables or disables sampler replacement. If not specified this
  # will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for
  # train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,
  # you can set ``replace_sampler_ddp=False`` and add your own distributed sampler. (type: bool, default: True)
  replace_sampler_ddp: true

  # Enable anomaly detection for the autograd engine.
  # Default: ``False``. (type: bool, default: False)
  detect_anomaly: false

  # If set to True, will `initially` run a batch size
  # finder trying to find the largest batch size that fits into memory.
  # The result will be stored in self.batch_size in the LightningModule.
  # Additionally, can be set to either `power` that estimates the batch size through
  # a power search or `binsearch` that estimates the batch size through a binary search.
  # Default: ``False``. (type: Union[str, bool], default: False)
  auto_scale_batch_size: false

  # If True, each LOCAL_RANK=0 will call prepare data.
  # Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data

  # .. deprecated:: v1.5
  #     Deprecated in v1.5.0 and will be removed in v1.7.0
  #     Please set ``prepare_data_per_node`` in ``LightningDataModule`` and/or
  #     ``LightningModule`` directly instead. (type: Union[bool, null], default: null)
  prepare_data_per_node:

  # Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.
  # Default: ``None``. (type: Union[Strategy, PrecisionPlugin, ClusterEnvironment, CheckpointIO, LayerSync, str, List[Union[Strategy, PrecisionPlugin, ClusterEnvironment, CheckpointIO, LayerSync, str]], null], default: null, known subclasses: pytorch_lightning.strategies.DDPStrategy, pytorch_lightning.strategies.BaguaStrategy, pytorch_lightning.strategies.DDP2Strategy, pytorch_lightning.plugins.DDP2Plugin, pytorch_lightning.strategies.DeepSpeedStrategy, pytorch_lightning.plugins.DeepSpeedPlugin, pytorch_lightning.strategies.DDPFullyShardedStrategy, pytorch_lightning.plugins.DDPFullyShardedPlugin, pytorch_lightning.strategies.HPUParallelStrategy, pytorch_lightning.strategies.DDPShardedStrategy, pytorch_lightning.plugins.DDPShardedPlugin, pytorch_lightning.plugins.DDPPlugin, pytorch_lightning.strategies.DDPSpawnStrategy, pytorch_lightning.strategies.DDPSpawnShardedStrategy, pytorch_lightning.plugins.DDPSpawnShardedPlugin, pytorch_lightning.strategies.TPUSpawnStrategy, pytorch_lightning.plugins.TPUSpawnPlugin, pytorch_lightning.plugins.DDPSpawnPlugin, pytorch_lightning.strategies.DataParallelStrategy, pytorch_lightning.plugins.DataParallelPlugin, pytorch_lightning.strategies.HorovodStrategy, pytorch_lightning.plugins.HorovodPlugin, pytorch_lightning.strategies.IPUStrategy, pytorch_lightning.plugins.IPUPlugin, pytorch_lightning.strategies.SingleDeviceStrategy, pytorch_lightning.strategies.SingleHPUStrategy, pytorch_lightning.strategies.SingleTPUStrategy, pytorch_lightning.plugins.SingleTPUPlugin, pytorch_lightning.plugins.SingleDevicePlugin, pytorch_lightning.plugins.PrecisionPlugin, pytorch_lightning.plugins.precision.MixedPrecisionPlugin, pytorch_lightning.plugins.ApexMixedPrecisionPlugin, pytorch_lightning.plugins.NativeMixedPrecisionPlugin, pytorch_lightning.plugins.ShardedNativeMixedPrecisionPlugin, pytorch_lightning.plugins.FullyShardedNativeMixedPrecisionPlugin, pytorch_lightning.plugins.DeepSpeedPrecisionPlugin, pytorch_lightning.plugins.DoublePrecisionPlugin, pytorch_lightning.plugins.HPUPrecisionPlugin, pytorch_lightning.plugins.IPUPrecisionPlugin, pytorch_lightning.plugins.TPUPrecisionPlugin, pytorch_lightning.plugins.TPUBf16PrecisionPlugin, pytorch_lightning.plugins.environments.BaguaEnvironment, pytorch_lightning.plugins.environments.KubeflowEnvironment, pytorch_lightning.plugins.environments.LightningEnvironment, pytorch_lightning.plugins.environments.LSFEnvironment, pytorch_lightning.plugins.environments.SLURMEnvironment, pytorch_lightning.plugins.environments.TorchElasticEnvironment, pytorch_lightning.plugins.TorchCheckpointIO, pytorch_lightning.plugins.HPUCheckpointIO, pytorch_lightning.plugins.XLACheckpointIO, pytorch_lightning.plugins.NativeSyncBatchNorm)
  plugins:

  # The mixed precision backend to use ("native" or "apex").
  # Default: ``'native''``. (type: str, default: native)
  amp_backend: native

  # The optimization level to use (O1, O2, etc...). By default it will be set to "O2"
  # if ``amp_backend`` is set to "apex". (type: Union[str, null], default: null)
  amp_level:

  # Whether to force internal logged metrics to be moved to cpu.
  # This can save some gpu memory, but can make training slower. Use with attention.
  # Default: ``False``. (type: bool, default: False)
  move_metrics_to_cpu: false

  # How to loop over the datasets when there are multiple train loaders.
  # In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,
  # and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets
  # reload when reaching the minimum length of datasets.
  # Default: ``"max_size_cycle"``. (type: str, default: max_size_cycle)
  multiple_trainloader_mode: max_size_cycle

  # Whether to use `Stochastic Weight Averaging (SWA)
  # <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>`_.
  # Default: ``False``.

  # .. deprecated:: v1.5
  #     ``stochastic_weight_avg`` has been deprecated in v1.5 and will be removed in v1.7.
  #     Please pass :class:`~pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging`
  #     directly to the Trainer's ``callbacks`` argument instead. (type: bool, default: False)
  stochastic_weight_avg: false

  # If set to True, will terminate training (by raising a `ValueError`) at the
  # end of each training batch, if any of the parameters or the loss are NaN or +/-inf.

  # .. deprecated:: v1.5
  #     Trainer argument ``terminate_on_nan`` was deprecated in v1.5 and will be removed in 1.7.
  #     Please use ``detect_anomaly`` instead. (type: Union[bool, null], default: null)
  terminate_on_nan:
